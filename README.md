# Human-Action-Estimation
## Introduction

Human action estimation plays a pivotal role in various fields, including surveillance, healthcare, human-computer interaction, and robotics. Accurately identifying and understanding human actions from self-taken image sequences is a meaningful yet crucial task. With recent advancements in deep learning, particularly the emergence of Vision Transformer (ViT) models, there has been a notable shift in utilizing self-attention mechanisms to capture spatial relationships in images, previously dominated by convolutional neural networks (CNNs).

This project focuses on leveraging deep learning methodologies to address the task of human action estimation. The primary objective is to develop a robust and efficient program capable of accurately recognizing and classifying a diverse range of human actions from sequences of images. The recognition of actions including walking, learning, and fitness. These kinds of routine-life movements hold immense potential for applications in fields like surveillance for abnormal behavior detection, healthcare for monitoring patient activities, and interactive systems for gesture-based control.

The project utilizes large-scale image datasets encompassing diverse human actions performed in various settings and environments. Vision Transformer architectures will be adapted and fine-tuned to process temporal sequences efficiently, leveraging techniques such as sequence embedding, positional encoding, and self-attention mechanisms to capture spatiotemporal features crucial for accurate action recognition.

Moreover, this report aims to document the methodologies employed, challenges encountered, experimental configurations, ViT architectures, hyperparameter tuning strategies, and evaluation metrics used to assess the performance of the Vision Transformer-based models. Evaluation metrics will include accuracy, temporal consistency, and other performance indicators to provide insights into the ViT models' effectiveness and their applicability to human action recognition tasks.

In conclusion, this research endeavors to investigate the potential and limitations of Vision Transformer models in the domain of human action recognition from image data. The outcomes and insights obtained aim to contribute to the advancement of deep learning methodologies for image analysis, specifically in understanding and classifying human actions for real-world applications.
